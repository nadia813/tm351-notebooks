{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "# Term frequency and inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final Notebook looking at how to handle textual data, we will consider how *inverse document frequency* is used to weight terms in the term frequency vectors. We will use the same training and test documents as in [the previous Notebook](22.3 Applying the classifier to a real dataset.ipynb) so that we can compare the performance of the two techniques directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the module material we discuss the technique of inverse document frequency weighting as well as stopword removal. Although we will not look at stopword removal in the Notebooks, while working through this Notebook you should think about how different techniques can be used to improve the performance of your data investigations. When working on your own investigation, you should always be thinking about how you would go about selecting different ways of treating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Initial imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "\n",
    "import math, string\n",
    "import os \n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same definitions for the main functions as in [Notebook 22.3](22.3 Applying the classifier to a real dataset.ipynb). In this case, we will use `tokenise_email_document` again, rather than the simpler `tokenise_document`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenise_email_document(emailDocIn_txt):\n",
    "    '''Convert an input string to a list of tokens using the operations:\n",
    "    \n",
    "        - convert to lower case\n",
    "        - split on whitespace\n",
    "        - remove surrounding punctuation\n",
    "    '''\n",
    "    return [token.strip(string.punctuation)  # remove punctuation around tokens\n",
    "            \n",
    "            for token in emailDocIn_txt.lower().split()] # Convert to lower case and split\n",
    "                                                         # on whitespace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def build_term_index(tokenisedDocuments_coll):\n",
    "    '''Return a set of all the terms appearing in the \n",
    "       documents in tokenisedDocuments_coll\n",
    "    '''\n",
    "    allTerms_set = set()  # Store the tokens as a set to remove repetitions\n",
    "    \n",
    "    for tokens_coll in tokenisedDocuments_coll:\n",
    "        allTerms_set = allTerms_set.union(set(tokens_coll))\n",
    "        \n",
    "    return list(allTerms_set)     # Return the members as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def build_tf_vector(tokenisedDocument_ls, termIndex_ls):\n",
    "    '''Return a pandas Series representing the term \n",
    "       frequency vector of the tokenised document \n",
    "       tokenisedDocument_ls, and indexed with termIndex_ls\n",
    "    '''\n",
    "    \n",
    "    return pd.Series(Counter(tokenisedDocument_ls),\n",
    "                     index=termIndex_ls).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to import the same training and test data as for the previous Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, we are using 1000 ham documents and 1000 spam documents as training data.\n",
    "\n",
    "The ham training data can be found in the folder:\n",
    "\n",
    "    data/trainingData/ham/\n",
    "    \n",
    "and the spam training data in the folder:\n",
    "\n",
    "    data/trainingData/spam/\n",
    "   \n",
    "\n",
    "We have also selected 200 ham documents and 200 spam documents to use as test data.\n",
    "\n",
    "The ham test data can be found in the folder:\n",
    "\n",
    "    data/testData/ham/\n",
    "    \n",
    "and the spam test data in the folder:\n",
    "\n",
    "    data/testData/spam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "trainingCorpusDocuments_ls = []\n",
    "trainingCorpusClasses_ls = []\n",
    "\n",
    "# First collect the ham documents:\n",
    "print(\"Reading ham training files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/trainingData/ham/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "            docText = fileIn.read()\n",
    "            docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "            \n",
    "            trainingCorpusDocuments_ls.append(docText)\n",
    "            trainingCorpusClasses_ls.append('ham')\n",
    "\n",
    "# Next, collect the spam documents:\n",
    "print(\"Reading spam training files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/trainingData/spam/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            pass\n",
    "        else:\n",
    "            with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "                docText = fileIn.read()\n",
    "                docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "\n",
    "                trainingCorpusDocuments_ls.append(docText)\n",
    "                trainingCorpusClasses_ls.append('spam')\n",
    "\n",
    "print('{} ham training files read'.format(trainingCorpusClasses_ls.count('ham')))\n",
    "print('{} spam training files read'.format(trainingCorpusClasses_ls.count('spam')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "testCorpusDocuments_ls = []\n",
    "testCorpusClasses_ls = []\n",
    "\n",
    "# First collect the ham documents:\n",
    "print(\"Reading ham test files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/testData/ham/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "            docText = fileIn.read()\n",
    "            docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "            \n",
    "            testCorpusDocuments_ls.append(docText)\n",
    "            testCorpusClasses_ls.append('ham')\n",
    "            \n",
    "# Next, collect the spam documents:\n",
    "print(\"Reading spam test files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/testData/spam/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            pass\n",
    "        else:\n",
    "            with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "                docText = fileIn.read()\n",
    "                docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "\n",
    "                testCorpusDocuments_ls.append(docText)\n",
    "                testCorpusClasses_ls.append('spam')\n",
    "\n",
    "print('{} ham test files read'.format(testCorpusClasses_ls.count('ham')))\n",
    "print('{} spam test files read'.format(testCorpusClasses_ls.count('spam')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will use the `tokenise_email_document` to perform the tokenisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "trainingTokenisedDocuments_ls = [tokenise_email_document(doc_txt) for doc_txt in trainingCorpusDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an inverse document frequency index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having imported and tokenised the data, we now need to build the inverse document frequency index. Recall that the definition of inverse document frequency (idf) for some term is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(term)=\\log_e\\left(\\frac{\\textrm{total number of documents}}{\\textrm{number of documents containing }term}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the term frequency index we built in Notebook 22.3, we can build a *pandas* Series which contains the inverse document frequency values for all the terms in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the term index of all the terms which appear in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "trainingTermIndex_ls = build_term_index(trainingTokenisedDocuments_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want a Series which represents how many documents each term appears in (the 'number of documents containing `term`' in the definition of `idf`). We will start with a Series whose index is the terms which appear in the collection, and which has zero for each document frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "documentFrequencyIndex_ss = pd.Series(0, index=trainingTermIndex_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can populate the Series with the document frequency count for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "for tokenisedDoc_ls in trainingTokenisedDocuments_ls:\n",
    "    for term in set(tokenisedDoc_ls):\n",
    "        documentFrequencyIndex_ss[term] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, to find out how many documents the term *bill* appears in, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "documentFrequencyIndex_ss['bill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can now create the idf index by dividing the number of documents in the training corpus by the document frequency, and using `np.log` to find the log of the values in the Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "idfIndex_ss = pd.Series(len(trainingCorpusDocuments_ls),  # Put the number of documents as\n",
    "                        index=trainingTermIndex_ls)       # each value\n",
    "\n",
    "idfIndex_ss = np.log(idfIndex_ss / documentFrequencyIndex_ss)  # Divide by the document \n",
    "                                                               # frequency and take the log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the impact that different terms will have. Comparing the inverse document frequency values of the terms *the* and *bill*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(idfIndex_ss['the'])\n",
    "print(idfIndex_ss['bill'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows that each occurence of *bill* will be much more heavily weighted than each occurrence of *the*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the training data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will quickly run into memory problems if we try to create a DataFrame containing the complete set of training documents, so as before we will only use the most common terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "termFrequencyIndex_ss = pd.Series(0, index=idfIndex_ss.index)\n",
    "\n",
    "for tokenisedDoc_ls in trainingTokenisedDocuments_ls:\n",
    "    for token in tokenisedDoc_ls:\n",
    "        termFrequencyIndex_ss[token] += 1\n",
    "\n",
    "termFrequencyIndex_ss.sort_values(ascending=False, inplace=True)\n",
    "        \n",
    "termFrequencyIndex_ss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, take the 200 most common terms and create an index containing only those terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "shortTermIndex = termFrequencyIndex_ss.index[:200]\n",
    "\n",
    "shortTermIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the `reindex` method to reduce the size of `idfIndex_ss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "idfIndex_ss = idfIndex_ss.reindex(shortTermIndex)\n",
    "\n",
    "idfIndex_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our set of training vectors. Previously, we used the term frequency for each term in the sentence. In this case, we multiply the term frequency by the inverse document frequency value for that term (to give tf.idf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "trainingTfIdfVectors_ls = [build_tf_vector(tokenisedDoc_ls, shortTermIndex) * idfIndex_ss\n",
    "                           for tokenisedDoc_ls in trainingTokenisedDocuments_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "trainingData_df = pd.DataFrame(trainingTfIdfVectors_ls)\n",
    "\n",
    "trainingData_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before, use this DataFrame and the training classes to build a *k*-NN classifier. Again, we will use *k*=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "spamFilter3_knn = KNeighborsClassifier(n_neighbors=3, metric='cosine', algorithm='brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "spamFilter3_knn.fit(trainingData_df,\n",
    "                    trainingCorpusClasses_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classifier to classify test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify the test data, we need the tf.idf vector for each vector in the test set. First tokenise the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "testTokenisedDocuments_ls = [tokenise_email_document(doc_txt) for doc_txt in testCorpusDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and convert to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "testTfIdfVectors_ls = [build_tf_vector(tokenisedDoc_ls, shortTermIndex) * idfIndex_ss\n",
    "                       for tokenisedDoc_ls in testTokenisedDocuments_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "testData_df = pd.DataFrame(testTfIdfVectors_ls)\n",
    "\n",
    "testData_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apply the classifier to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'predicted':spamFilter3_knn.predict(testData_df),\n",
    "                           'actual':testCorpusClasses_ls})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can use the `pd.crosstab` function to present the results in a more readable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tabulatedResults_df = pd.crosstab(results_df.predicted, results_df.actual, margins=True)\n",
    "\n",
    "tabulatedResults_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can now print the results, and give an overall percentage accuracy (total number of emails that were correctly classified into *ham* or *spam*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print('Ham correctly classified as ham: {}/{}'.format(tabulatedResults_df['ham']['ham'],\n",
    "                                                      tabulatedResults_df['ham']['All']))\n",
    "\n",
    "print('Ham incorrectly classified as spam: {}/{}'.format(tabulatedResults_df['ham']['spam'],\n",
    "                                                         tabulatedResults_df['ham']['All']))\n",
    "\n",
    "print('Spam incorrectly classified as ham: {}/{}'.format(tabulatedResults_df['spam']['ham'],\n",
    "                                                         tabulatedResults_df['spam']['All']))\n",
    "\n",
    "print('Spam correctly classified as spam: {}/{}'.format(tabulatedResults_df['spam']['spam'],\n",
    "                                                        tabulatedResults_df['spam']['All']))\n",
    "\n",
    "print('Overall system accuracy: {:.1%}'.format((tabulatedResults_df['ham']['ham'] + \n",
    "                                                tabulatedResults_df['spam']['spam']) / \n",
    "                                                     tabulatedResults_df['All']['All']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an (even) stronger result than our previous attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key message to take away here is that we have managed to greatly improve the behaviour of our data application by considering the nature of the data (natural language documents), choosing an appropriate similarity measure (cosine similarity) and using knowledge of the dataset to improve the way the data is processed (tf.idf measures) in a way that is appropriate for the particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "\n",
    "If you are working through this set of Notebooks as a whole, you've completed the Part 22 Notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
