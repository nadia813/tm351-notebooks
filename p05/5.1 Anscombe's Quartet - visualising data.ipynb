{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anscombe's Quartet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a 1973 paper, 'Graphs in Statistical Analysis', published in *The American Statistician*, vol. 27, no. 1. (February 1973), pp. 17-21, statistician Francis Anscombe provided the briefest of abstracts: 'Graphs are essential to good statistical analysis.  Ordinary scatterplots and \"triple\" scatterplots are discussed in relation to regression analysis'\n",
    "\n",
    "His paper opened with a brief meditation on *the usefulness of graphs*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most textbooks on statistical methods, and most statistical computer programs, pay too little attention to graphs. Few of us escape being indoctrinated with these notions:\n",
    "\n",
    "> 1. numerical calculations are exact, but graphs are rough; \n",
    "> 2. for any particular kind of statistical data there is just one set of calculations constituting a correct statistical analysis; \n",
    "> 3. performing intricate calculations is virtuous, whereas actually looking at the data is cheating.\n",
    "\n",
    "> A computer should make _both_ calculations _and_ graphs. Both sorts of output should be studied; each will contribute to understanding. \n",
    "\n",
    "> Graphs can have various purposes, such as: (i) to help us perceive and appreciate some broad features of the data, (ii) to let us look behind those broad features and see what else is there. Most kinds of statistical calculation rest on assumptions about the behavior of the data. Those assumptions may be false, and then the calculations may be misleading. We ought always to try to check whether the assumptions are reasonably correct; and if they are wrong we ought to be able to perceive in what ways they are wrong. Graphs are very valuable for these purposes.\n",
    "\n",
    "> Good statistical analysis is not a purely routine matter, and generally calls for more than one pass through the computer. The analysis should be sensitive both to peculiar features in the given numbers and also to whatever background information is available about the variables. The latter is particularly helpful in suggesting alternative ways of setting up the analysis. Thought and ingenuity devoted to devising good graphs are likely to pay off. Many ideas can be gleaned from the literature..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate his call to arms, Anscombe generated a set of four simple pairwise (*x* and *y* values) datasets (known as datasets `I`, `II`, `III`, `IV`) intended to demonstrate the importance and usefulness of actually looking at graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore Ancombe's Quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the data for Anscome's Quartet datasets from a data file into a hierarchically indexed DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "aq_df = pd.read_csv('data/AnscombesDataSets/anscombesQuartet_hier.csv',\n",
    "                  header=[0,1], index_col=[0])\n",
    "aq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Anscombe's Quartet with statistical tools\n",
    "Let's do what Anscombe argues we shouldn't do: let's simply use the statistics to speak about the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual summary statistical properties, the mean and standard deviation (variance), of datasets I to IV hardly vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "aq_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "aq_df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average (mean) values are almost identical for *x* and *y* across the groups and the variances within each group are indistinguishable down to fine tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other statistical properties, including some statistical visualisations such as regression lines, were also the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the *seaborn* package to plot the linear regression lines for each of the four datasets.\n",
    "\n",
    "In this visualisation we're _not_ looking at the dataset itself; we're looking at the result (the output) of a linear regression statistical analysis applied to the dataset.\n",
    "\n",
    "Note: seaborn is a package specifically intended to support statistical data visualisation.  \n",
    "\n",
    "Sadly the seaborn library is giving a deprecated warning each time it is used.  It looks like we froze our Virtual Machine after one the packages seaborn makes use of was changed, but before the seaborn distribution caught up.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "# Load the example dataset for Anscombe's Quartet from the seaborn package;\n",
    "#   the DataFrame that results has a different shape than the hierarchical \n",
    "#   DataFrame we read in earlier. \n",
    "# This is a long thin DataFrame in which the column 'dataset' contains a value marker to \n",
    "#   show which dataset each row belongs to.  The values in each dataset are identical to those \n",
    "#   we used earlier.\n",
    "anscombe_df = sns.load_dataset('anscombe')\n",
    "anscombe_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# seaborn uses a FacetGrid class to create multiple charts in one figure. \n",
    "#  We can use the column 'dataset' values to determine the different x,y values for each plot.\n",
    "#  All the x and y values lie between 0 and 16, so we can use those for the plot limits.\n",
    "# The following sets up the FaceGrid over the anscome_df DataFrame.\n",
    "fourplots = sns.FacetGrid(anscombe_df, \n",
    "                          col='dataset', \n",
    "                          col_wrap=2, \n",
    "                          xlim=(0,16), ylim=(0,16))\n",
    "\n",
    "# And then we can map (chart) these using the regression line plot \n",
    "#  (for now we don't want the underlying data points plotted, so we set scatter to False,\n",
    "#   and we don't want to show the confidence interval for the regression line.)\n",
    "fourplots.map(sns.regplot, 'x', 'y', scatter=False, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from these summary statistics - _mean_, _variance_ and _linear regression_ - we could be led to conclude that the datasets are, to all intents and purposes, the same (or at least, very similar).\n",
    "\n",
    "But, we've another statistical tool that we can add to the regression line - we can show the confidence interval, an indication of how well the regression line fits the data.  Let's plot the 95% confidence interval on our regression lines (`ci=95` in the code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Show the results of a linear regression within each dataset, \n",
    "# with the 95% confidence interval overlayed.\n",
    "fourplots = sns.FacetGrid(anscombe_df, \n",
    "                          col='dataset', \n",
    "                          col_wrap=2, \n",
    "                          xlim=(0,16), ylim=(0,16))\n",
    "\n",
    "fourplots.map(sns.regplot, 'x', 'y', scatter=False, ci=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're beginning to see some differences in our statistical measures: there may be something that needs exploring further, but the differences are not that great and very easy to overlook.  \n",
    "\n",
    "So maybe we simply conclude that datasets I through IV are similar, but a little different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Anscombe's Quartet directly with visualisation tools \n",
    "Now, let's follow Anscombe's advice and look at our dataset with a simple graph, no analysis, just looking at the 'raw' data values.  Note: we'll use the original hierarchical DataFrame, `aq_df`, here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most natural way to look at this data, a series of points, is to use a *scatter plot*, with the *x*-values placed along the horizontal axis and the *y*-values along the vertical axis. Points are plotted as marks using their *x* and *y* values within each group, as the Cartesian coordinates for each point.\n",
    "\n",
    "Using `pandas.plot()`, we can construct the plot directly from the DataFrame, if the DataFrame is correctly shaped.  We'll look at `pandas.plot()` and the `matplotlib` visualisation library in the following Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way of plotting the data is to generate a chart separately for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the group.\n",
    "group = 'I'\n",
    "# Specify the columns.\n",
    "cols = ['x', 'y']\n",
    "# Generate a DataFrame containing just the required columns from the specified group,\n",
    "group1 = aq_df[group][cols]\n",
    "#   which looks like:\n",
    "group1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "And using the `pandas.plot.scatter()` method we can get a basic scatter plot for that data group.  (We'll reduce it in size to make it easier to view.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# In practice I played around with the parameters until I got a reasonably pleasing plot.\n",
    "group1.plot.scatter(x='x', y='y',\n",
    "                    figsize=(4,4), \n",
    "                    title='Dataset I', \n",
    "                    xlim=(0,16), ylim=(0,16) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to extract the data from our DataFrame, and what plot parameters give a reasonable view of the data, we can generate a plot for each dataset in turn. Here I'm using a list of the dataset names as the iterator with the iterator body being a simple `plot.scatter()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Loop through the group labels, put that group's x and y values \n",
    "# into a scatter plot, and make them small enough to see all \n",
    "# four datasets at the same time.\n",
    "for i in ['I','II','III','IV']:\n",
    "    aq_df[i].plot.scatter(x='x', y='y', title=('Dataset: '+ i), \n",
    "                          figsize=(2,2), xlim=(0,16), ylim=(0,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally see how different datasets I, II, III and IV actually are, each with its own story to tell, but not stories we would have been alerted to if we had rushed headlong into the analysis using the summary statistics alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anscombe's Quartet, though only a small dataset, offers a salutary lesson. The summary statistics for the _x_ and _y_ values across each group may be the same, and a quick look at the values in the data tables makes it hard to picture with any degree of certainity, but when visualised as a whole, each group of data clearly tells a different story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anscombe concluded his paper as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Graphical output such as described above is readily available to anyone who does his own programming. I myself habitually generate such plots at an APL terminal, and have come to appreciate their importance. A skilled Fortran or PL/1 programmer, with an organized library of subroutines, can do the same (on a larger scale). \n",
    "\n",
    "> Unfortunately, most persons who have recourse to a computer for statistical analysis of data are not much interested either in computer programming or in statistical method, being primarily concerned with their own proper business. Hence the common use of library programs and various statistical packages. Most of these originated in the pre-visual era. The user is not showered with graphical displays. He can get them only with trouble, cunning and a fighting spirit. It's time that was changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational techniques have moved on somewhat since 1973, of course, and times have indeed changed. Graphical displays are everywhere, and libraries such as matplotlib, ggplot, and seaborn mean that we are now in a position to 'write' very powerful expressions that can generate statistical graphics for us, directly from a cleaned and prepared dataset, using just a few well-chosen phrases. But getting the data into the right shape may still require significant amounts of *trouble, cunning and a fighting spirit*.\n",
    "\n",
    "This Notebook should be a valuable reminder that one of the first things you should do with a dataset is visualise it - then remember to check that any subsequent analysis makes sense for a dataset with that shape.\n",
    "\n",
    "May the visualisations begin ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now. \n",
    "\n",
    "If you are working through this set of Notebooks as a whole, move on to `05.2 Getting started with maps - folium`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
