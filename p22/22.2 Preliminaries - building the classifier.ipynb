{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "source": [
    "# Preliminaries: building a *k*-nearest neighbours classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [the previous Notebook](22.1 Case study preliminaries - the vector space model.ipynb) we saw how to represent a collection of documents as term frequency vectors using Python, and how cosine distance can be used to estimate the similarity between pairs of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will build a simple *k*-nearest neighbours classifier which will form the basis of our spam filter. Be aware that in these Notebooks we are *not* attempting to make the code as efficient as possible. Rather, we are aiming to show how an intuitively simple algorithm, such as *k*-NN, can be used to deliver good results on a practical, real-world application. If you were to try to use these techniques on a genuinely large dataset (such as on all the email sent and received by a large corporation in a typical day), you would expect to need a significant amount of time exploring techniques to optimise your implementations of the algorithms (or, of course, to use an off-the-shelf implementation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should spend around an hour on this Notebook, and around a further 40 minutes on the iCMA questions referenced at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial imports and function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook the functions we use are just the same as those defined in [Notebook 22.1](22.1 Case study preliminaries - the vector space model.ipynb). We will also be using the `KNeighborsClassifier` library which we explored in Part 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function definitions are exactly those which we developed in [Notebook 22.1](22.1 Case study preliminaries - the vector space model.ipynb). If you need a reminder of how they are used, you should reread that Notebook, in particular the section entitled 'Bringing it all together: estimating document similarity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenise_document(docIn_str):\n",
    "    '''Return a list of the tokens in the input string docIn_str'''\n",
    "    return docIn_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_term_index(tokenisedDocuments_coll):\n",
    "    '''Return a set of all the terms appearing in the \n",
    "       documents in tokenisedDocuments_coll\n",
    "    '''\n",
    "    allTerms_set = set()  # Store the tokens as a set to remove repetitions\n",
    "    \n",
    "    for tokens_coll in tokenisedDocuments_coll:\n",
    "        allTerms_set = allTerms_set.union(set(tokens_coll))\n",
    "        \n",
    "    return list(allTerms_set)     # Return the members as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tf_vector(tokenisedDocument_ls, termIndex_ls):\n",
    "    '''Return a pandas Series representing the term \n",
    "       frequency vector of the tokenised document \n",
    "       tokenisedDocument_ls, and indexed with termIndex_ls\n",
    "    '''\n",
    "    \n",
    "    return pd.Series(Counter(tokenisedDocument_ls),\n",
    "                     index=termIndex_ls).fillna(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "## Defining some toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide some initial data to help us build the classifier, we will use some invented sentences, which can be divided into two categories: *ham* and *spam*. The aim of the classifier will be to estimate whether a new document is more similar to the *ham* documents, or to the *spam* documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ham documents:\n",
    "    \n",
    "1. *details of the meeting will be sent by email*\n",
    "\n",
    "2. *we anticipate a big financial loss*\n",
    "\n",
    "3. *we need to reduce the size of the workforce*\n",
    "\n",
    "4. *the financial meeting aims to win over investors*\n",
    "\n",
    "Spam documents:\n",
    "    \n",
    "5. *reduce your hair loss now*\n",
    "\n",
    "6. *enter the prize draw and win big*\n",
    "\n",
    "7. *email us for details of this unique opportunity*\n",
    "\n",
    "8. *your bank account will be suspended*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous Notebook, we will represent each of these documents using a Python dictionary, where the keys in the dictionaries are terms, and the values are counts of the term in the document.\n",
    "\n",
    "We will also set up two lists of values: the first will contain the documents themselves, and the second the classes of those documents. Then the *n*th member of the list of documents will have the class of the *n*th member of the list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingDocuments_ls = ['details of the meeting will be sent by email',\n",
    "                        'we anticipate a big financial loss',                 \n",
    "                        'we need to reduce the size of the workforce',                \n",
    "                        'the financial meeting aims to win over investors',\n",
    "                        'reduce your hair loss now', \n",
    "                        'enter the prize draw and win big',\n",
    "                        'email us for details of this unique opportunity',                 \n",
    "                        'your bank account will be suspended']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And set up another list to represent the document classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingClasses_ls = ['ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a DataFrame of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you worked through Notebook `20.1 The k-nearest neighbours classifier` you saw that we can use a *pandas* DataFrame as the input to the classifier. In this section of the Notebook we will build a DataFrame of the training documents in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are aiming for is a DataFrame whose rows represent the training documents and whose columns represent the documents' features: in this case, the terms in each document. So we are aiming for a DataFrame which looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||details|of|we|anticipate| ... |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**doc 1**|1|1|0|0| ... |\n",
    "|**doc 2**|0|0|1|1| ... |\n",
    "| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each column of a DataFrame object is a Series object, it is straightforward to build the training data DataFrame from the Series objects returned by the `build_tf_vector` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same procedure as in [Notebook 22.1](22.1 Case study preliminaries - the vector space model.ipynb), our first task is to convert the list of training documents into a list of tokenised documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenisedTrainingDocuments_ls = [tokenise_document(doc_str) for doc_str in trainingDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *n*th member of `tokenisedTrainingDocuments_ls` is a tokenised form of the *n*th member of `trainingDocuments_ls`. For example, choosing an arbitrary member of the lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3   # arbitrary choice, can have 0 <= n <= 7\n",
    "print(trainingDocuments_ls[n])\n",
    "print()\n",
    "print(tokenisedTrainingDocuments_ls[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we use the `build_term_index` function and the collection of tokenised documents to build a term index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "termIndex_ls = build_term_index(tokenisedTrainingDocuments_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     2,
     18
    ]
   },
   "source": [
    "and use this to convert the tokenised documents to term vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingTfVectors_ls = [build_tf_vector(tokenisedDoc_ls, termIndex_ls)\n",
    "                        for tokenisedDoc_ls in tokenisedTrainingDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the documents are represented as a list of term vectors, we can convert these to a DataFrame by simply calling the `DataFrame` constructor on the list of term vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_df = pd.DataFrame(trainingTfVectors_ls)\n",
    "\n",
    "trainingData_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in which the *n*th row encodes the term vector representing the *n*th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the classifier in the same was as in Notebook 20.1. As before, we will use a *k*-NN classifier, with, in this case, *k*=3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spamFilter3_knn = KNeighborsClassifier(n_neighbors=3, metric='cosine', algorithm='brute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have set the metric to `cosine` so that the algorithm will use cosine distance rather than Euclidean distance. The option `algorithm='brute'` is a quirk which tells the learner to use the (least efficient) brute-force algorithm in training the classifier: there are various algorithms which improve the training efficiency for Euclidean distance which cannot be used for cosine distance, and so this option is simply telling the learner not to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamFilter3_knn.fit(trainingData_df,\n",
    "                    trainingClasses_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classifier to classify test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained classifier, we can use it to attempt to classify some new documents. Suppose we have the following documents:\n",
    "\n",
    "1. *the investors will email this opportunity to the workforce*\n",
    "2. *please email us your bank details to win*\n",
    "\n",
    "We would hope that the spam filter might classify the first example as ham, and the second as spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify these cases, we need to construct a further DataFrame containing the test data. We follow the same sequence as before (although we use the same term index for the test data as the training data, to ensure that the term vectors align)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a collection of test documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDocuments_ls = ['the investors will email this opportunity to the workforce',\n",
    "                    'please email us your bank details to win']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, tokenise the test documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTokenisedDocuments_ls = [tokenise_document(testDoc_str)\n",
    "                             for testDoc_str in testDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the tokenised test documents to term frequency vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTfVectors_ls = [build_tf_vector(testDoc_str, termIndex_ls)\n",
    "                    for testDoc_str in testTokenisedDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally, convert the term vectors into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData_df = pd.DataFrame(testTfVectors_ls)\n",
    "\n",
    "testData_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the trained classifier to this data to see how the test data is classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamFilter3_knn.predict(testData_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first document has been classified as ham, and the second as spam, as we would have hoped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now built a simple, but working, spam filter which uses a collection of classified ham and spam documents to attempt to determine whether a new document is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "You are now ready to attempt Questions 5 and 6 of iCMA 46, which cover the material you have seen in this Notebook and the previous one. You should expect to spend around 10 minutes on Question 5, and around half an hour on Question 6.\n",
    "\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "\n",
    "If you are working through this set of Notebooks as a whole, move on to [`22.3 Applying the classifier to a real dataset`](22.3 Applying the classifier to a real dataset.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
