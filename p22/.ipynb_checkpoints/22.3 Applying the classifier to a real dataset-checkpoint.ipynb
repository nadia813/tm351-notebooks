{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "source": [
    "# Applying the classifier to a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the standard imports in this Notebook we will include `sklearn.neighbors.KNeighborsClassifier`, and the `collections.Counter` class, as used in the definition of `build_term_vector`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Notebook 22.1](22.1 Case study preliminaries - the vector space model.ipynb) and [Notebook 22.2](22.2 Preliminaries - building the classifier.ipynb) we saw how to represent a collection of textual documents, how to estimate the similarity between them, and how to use the documents and the similarity measure as a way of building a simple spam filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will apply the work from those two Notebooks to a collection of real emails, which have been classified as either ham or spam. By using a subset of the classified data, we will be able to see how well the classifier behaves on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless we state otherwise, the functions we use in this Notebook will be the same as those defined in the two previous notebooks. As in [Notebook 22.2](22.2 Preliminaries - building the classifier.ipynb) we will use the same functions as much as possible, but we will find that we need to adapt the techniques to overcome the difficulties of real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the same imports and function definitions as in [Notebook 22.2](22.2 Preliminaries - building the classifier.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "If you are unclear about how the following functions are used, you should reread the previous Notebooks to refresh your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenise_document(docIn_str):\n",
    "    '''Return a list of the tokens in the input string docIn_str'''\n",
    "    return docIn_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_term_index(tokenisedDocuments_coll):\n",
    "    '''Return a set of all the terms appearing in the \n",
    "       documents in tokenisedDocuments_coll\n",
    "    '''\n",
    "    allTerms_set = set()  # Store the tokens as a set to remove repetitions\n",
    "    \n",
    "    for tokens_coll in tokenisedDocuments_coll:\n",
    "        allTerms_set = allTerms_set.union(set(tokens_coll))\n",
    "        \n",
    "    return list(allTerms_set)     # Return the members as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tf_vector(tokenisedDocument_ls, termIndex_ls):\n",
    "    '''Return a pandas Series representing the term \n",
    "       frequency vector of the tokenised document \n",
    "       tokenisedDocument_ls, and indexed with termIndex_ls\n",
    "    '''\n",
    "    \n",
    "    return pd.Series(Counter(tokenisedDocument_ls),\n",
    "                     index=termIndex_ls).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training and test corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "source": [
    "So far, we have only applied the classifier to some toy data. In this Notebook we will use a subset of the Enron spam corpus to investigate how well the basic classifier works on real data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "Because the whole corpus is quite large, we have taken a random subset of the corpus so that we can carry out experiments more quickly. To reiterate what was said previously: the aim of this week's work is to illustrate how the techniques you have seen can be used in a practical application, rather than to give a complete account of how to do this in an efficient and scalable manner. \n",
    "\n",
    "All the documents are stored as text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "\n",
    "For this Notebook we have selected 1000 ham documents and 1000 spam documents as training data.\n",
    "\n",
    "The ham training data can be found in the folder:\n",
    "\n",
    "    data/trainingData/ham/\n",
    "    \n",
    "and the spam training data in the folder:\n",
    "\n",
    "    data/trainingData/spam/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "\n",
    "We have also selected 200 ham documents and 200 spam documents to use as test data.\n",
    "\n",
    "The ham test data can be found in the folder:\n",
    "\n",
    "    data/testData/ham/\n",
    "    \n",
    "and the spam test data in the folder:\n",
    "\n",
    "    data/testData/spam/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect the training corpus into a list of strings. Because the file structures of the email folders are not standardised, we will need to use the `os.walk` function to find all the text files in the folder hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingCorpusDocuments_ls = []\n",
    "trainingCorpusClasses_ls = []\n",
    "\n",
    "# First collect the ham documents:\n",
    "print(\"Reading ham files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/trainingData/ham/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(path, file), 'r') as fileIn:\n",
    "            trainingCorpusDocuments_ls.append(fileIn.read())\n",
    "            trainingCorpusClasses_ls.append('ham')\n",
    "            \n",
    "print('{numHamFiles} ham files read'.format(numHamFiles=len(trainingCorpusDocuments_ls)))\n",
    "\n",
    "# Next, collect the spam documents:\n",
    "print(\"Reading spam files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/trainingData/spam/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            pass\n",
    "        else:\n",
    "            with open(os.path.join(path, file), 'r') as fileIn:\n",
    "                trainingCorpusDocuments_ls.append(fileIn.read())\n",
    "                trainingCorpusClasses_ls.append('spam')\n",
    "            \n",
    "print('{numSpamFiles} ham files read'.format(numSpamFiles=len(trainingCorpusDocuments_ls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear!\n",
    "\n",
    "You should find that simply trying to read in the files as text has raised an error. It is characteristic of many data collections, but particularly text collections, that the documents will contain characters which cannot be parsed using an off-the-shelf text analyser such as the standard methods in Python. This is a practical occurrence of the issues you saw in Part 2, on file encoding, and Part 3 on data preparation.\n",
    "\n",
    "But this error raises the first of our decisions: what to do about unparsable characters?\n",
    "\n",
    "One possibility, given that we know we are dealing with corporate emails, would be to attempt to identify any known quirks of the email system used by that corporation. \n",
    "\n",
    "In this case, we will take a simple solution to the problem, and just ignore any unparseable characters. To do this, we take advantage of the `ignore` parameter in the python `decode` library. Of course, this might not be the best solution: perhaps the spam emails are more likely to contain unparseable characters than the ham emails, and knowing this could be a useful pointer towards which emails are spam. However, for the moment we will try just removing the offending characters.\n",
    "\n",
    "To do this, the files need to be read in binary format, and then decoded into utf-8 with the `ignore` parameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingCorpusDocuments_ls = []\n",
    "trainingCorpusClasses_ls = []\n",
    "\n",
    "# First collect the ham documents:\n",
    "print(\"Reading ham training files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/trainingData/ham/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "            docText = fileIn.read()\n",
    "            docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "            \n",
    "            trainingCorpusDocuments_ls.append(docText)\n",
    "            trainingCorpusClasses_ls.append('ham')\n",
    "\n",
    "# Next, collect the spam documents:\n",
    "print(\"Reading spam training files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/trainingData/spam/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            pass\n",
    "        else:\n",
    "            with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "                docText = fileIn.read()\n",
    "                docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "\n",
    "                trainingCorpusDocuments_ls.append(docText)\n",
    "                trainingCorpusClasses_ls.append('spam')\n",
    "\n",
    "print('{} ham training files read'.format(trainingCorpusClasses_ls.count('ham')))\n",
    "print('{} spam training files read'.format(trainingCorpusClasses_ls.count('spam')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded the training data, we can now load the test data in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testCorpusDocuments_ls = []\n",
    "testCorpusClasses_ls = []\n",
    "\n",
    "# First collect the ham documents:\n",
    "print(\"Reading ham test files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/testData/ham/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "            docText = fileIn.read()\n",
    "            docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "            \n",
    "            testCorpusDocuments_ls.append(docText)\n",
    "            testCorpusClasses_ls.append('ham')\n",
    "            \n",
    "# Next, collect the spam documents:\n",
    "print(\"Reading spam test files...\")\n",
    "\n",
    "for (path, dirs, files) in os.walk('./data/testData/spam/'):\n",
    "    \n",
    "    for file in files:\n",
    "        if file[0] == '.':  # Don't process hidden files\n",
    "            pass\n",
    "        else:\n",
    "            with open(os.path.join(path, file), 'rb') as fileIn:\n",
    "                docText = fileIn.read()\n",
    "                docText = docText.decode('utf-8', 'ignore')   # decoding the utf-8\n",
    "\n",
    "                testCorpusDocuments_ls.append(docText)\n",
    "                testCorpusClasses_ls.append('spam')\n",
    "\n",
    "print('{} ham test files read'.format(testCorpusClasses_ls.count('ham')))\n",
    "print('{} spam test files read'.format(testCorpusClasses_ls.count('spam')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of documents, and a list of their classification into *ham* or *spam*. For example, to see the training document with index 30, we can say:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingCorpusDocuments_ls[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to see its classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingCorpusClasses_ls[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Notebooks [22.1](22.1 Case study preliminaries - the vector space model.ipynb) and [22.2](22.2 Preliminaries - building the classifier.ipynb) we used a simple tokenisation technique of splitting on whitespace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the module materials we suggested that a better (although still far from perfect) tokenisation strategy for real text would be to:\n",
    "\n",
    "1. include all the metadata for each email\n",
    "2. assume that all the individual tokens in an email are separated by whitespace\n",
    "3. cast all the tokens into lower case\n",
    "4. remove all punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this strategy we will create a new tokenisation function, `tokenise_email_document`, to implement these stages. For punctuation, we will use Python's `string` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string     # A string representing punctuation characters\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenise_email_document(emailDocIn_txt):\n",
    "    '''Convert an input string to a list of tokens using the operations:\n",
    "    \n",
    "        - convert to lower case\n",
    "        - split on whitespace\n",
    "        - remove surrounding punctuation\n",
    "    '''\n",
    "    return [token.strip(string.punctuation)  # remove punctuation around tokens\n",
    "            \n",
    "            for token in emailDocIn_txt.lower().split()] # Convert to lower case and split\n",
    "                                                         # on whitespace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this function in action, let's call it on a document with mixed case and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenise_email_document('\"Hello!\" said John, loudly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare with the original tokenisation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenise_document('\"Hello!\" said John, loudly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new function is still very simplistic, but extends the original in a useful way. Tokenisation techniques can range from the simple (such as this) to the extremely complex, and for some languages, such as Chinese, a completely different approach is required. In practice, tokenisation techniques are often implemented using regular expressions, which strike a good balance between computational efficiency and appropriate expressive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a DataFrame of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now follow the same pattern of analysis as in [Notebook 22.2](22.2 Preliminaries - building the classifier.ipynb) by building a DataFrame with which to train the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the first task is to convert the collection of training documents into a list of tokenised documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingTokenisedDocuments_ls = [tokenise_email_document(doc_txt) for doc_txt in trainingCorpusDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *n*th member of `tokenisedDocuments_ls` is a tokenised form of the *n*th member of `trainingDocuments_ls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 30\n",
    "\n",
    "print(trainingCorpusDocuments_ls[n])\n",
    "print()\n",
    "print(trainingTokenisedDocuments_ls[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a term index, and use the `build_term_index` function to build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "termIndex_ls = build_term_index(trainingTokenisedDocuments_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same process as in the previous Notebooks, we would now convert the list of tokenised documents into a list of term vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>DO NOT RUN THE NEXT CELL!</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DO NOT RUN THIS CELL!!\n",
    "\n",
    "trainingTfVectors_ls = [build_term_vector(tokenisedDoc_ls, termIndex_ls)\n",
    "                        for tokenisedDoc_ls in trainingTokenisedDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the previous cell is just the same as we used in [Notebook 22.2](22.2 Preliminaries - building the classifier.ipynb), in this case the size of the index (which turns out to have around 100,000 entries) results in a memory error. (If you want to see what happens, feel free to change the cell to an executable code cell and run it, but on some machines it can take a very long time before an error is finally raised)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you might be thinking that this is a result of using a single computer, bear in mind that you are also using a very small dataset! For a company like Google, issues such as the most efficient selection of linguistic features is of critical importance, as small variations in the size of the indexing terms can have major knock-on effects on the (financial) cost of storage and the (processing) cost of data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, to reduce the size of the DataFrame to something managable, rather than use every term which appears in the index, we will use the most common terms which appear in the most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "termFrequencyIndex_ss = pd.Series(0, index=termIndex_ls)\n",
    "\n",
    "for tokenisedDoc_ls in trainingTokenisedDocuments_ls:\n",
    "    for token in tokenisedDoc_ls:\n",
    "        termFrequencyIndex_ss[token] += 1\n",
    "\n",
    "termFrequencyIndex_ss.sort_values(ascending=False, inplace=True)\n",
    "        \n",
    "termFrequencyIndex_ss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of `termFrequencyIndex_ss` now forms a list of the terms which appear in the training document collection, sorted by frequency in decreasing order. To consider the most common *n* terms in the dataset, we can use the first *n* members of `termFrequencyIndex_ss`'s index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try building the training DataFrame again, but this time we will use only a subset of all the terms as the index. Choosing an arbitrary length for the index, let's try building the training DataFrame with an index of the 200 most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shortTermIndex = termFrequencyIndex_ss.index[:200]\n",
    "\n",
    "shortTermIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL!!\n",
    "\n",
    "trainingTfVectors_ls = [build_tf_vector(tokenisedDoc_ls, shortTermIndex)\n",
    "                        for tokenisedDoc_ls in trainingTokenisedDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before, convert the training vectors into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData_df = pd.DataFrame(trainingTfVectors_ls)\n",
    "\n",
    "trainingData_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now use this DataFrame and the training classes to build a *k*-NN classifier. Again, we will use *k*=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spamFilter3_knn = KNeighborsClassifier(n_neighbors=3, metric='cosine', algorithm='brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spamFilter3_knn.fit(trainingData_df,\n",
    "                    trainingCorpusClasses_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classifier to classify test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this classifier to classify the test data, we need to convert the test documents to a DataFrame as well, using the same techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tokenise the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTokenisedData_ls = [tokenise_email_document(doc_txt) for doc_txt in testCorpusDocuments_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert the tokenised data to term frequency vectors using the same index as for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTfVectors_ls = [build_tf_vector(tokenisedDoc_ls, shortTermIndex)\n",
    "                    for tokenisedDoc_ls in testTokenisedData_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData_df = pd.DataFrame(testTfVectors_ls)\n",
    "\n",
    "testData_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'predicted':spamFilter3_knn.predict(testData_df),\n",
    "                           'actual':testCorpusClasses_ls})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluating the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have now constructed the necessary training set, test set, and classification function, and so we can apply the classifier to see how well it classifies emails into the spam and ham classes.\n",
    "\n",
    "To evaluate the technique, we will create a DataFrame in which the `actual` column contains the actual class of a test item, and the `predicted` column contains the class predicted by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate how well the basic spam filter works by using the cross tabulation functionality of a DataFrame (you saw crosstab tables in Part 4 and Notebook 04.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tabulatedResults_df = pd.crosstab(results_df.predicted, results_df.actual, margins=True)\n",
    "\n",
    "tabulatedResults_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the results, and give an overall percentage accuracy (total number of emails that were correctly classified into *ham* or *spam*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Ham correctly classified as ham: {}/{}'.format(tabulatedResults_df['ham']['ham'],\n",
    "                                                      tabulatedResults_df['ham']['All']))\n",
    "\n",
    "print('Ham incorrectly classified as spam: {}/{}'.format(tabulatedResults_df['ham']['spam'],\n",
    "                                                         tabulatedResults_df['ham']['All']))\n",
    "\n",
    "print('Spam incorrectly classified as ham: {}/{}'.format(tabulatedResults_df['spam']['ham'],\n",
    "                                                         tabulatedResults_df['spam']['All']))\n",
    "\n",
    "print('Spam correctly classified as spam: {}/{}'.format(tabulatedResults_df['spam']['spam'],\n",
    "                                                        tabulatedResults_df['spam']['All']))\n",
    "\n",
    "print('Overall system accuracy: {:.1%}'.format((tabulatedResults_df['ham']['ham'] + \n",
    "                                                tabulatedResults_df['spam']['spam']) / \n",
    "                                                     tabulatedResults_df['All']['All']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very good baseline result. In fact, this is much better than is typical for machine-learning systems; the small size of the dataset, and the fact that all the ham files come from within the same organisation makes this a much easier task than a full-blown spam filter for working on a wide range of emails.\n",
    "\n",
    "In the next and final Notebook of Part 22, we will look at using inverse document frequency measures to try to improve the performance of the spam filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What next?\n",
    "\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "\n",
    "If you are working through this set of Notebooks as a whole, move on to [`22.4 Term frequency and inverse document frequency`](22.4 Term frequency and inverse document frequency.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
